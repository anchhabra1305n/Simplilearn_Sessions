{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pPi899u95iC2"
   },
   "source": [
    "### <b> Ensemble Learning </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4IDGoFSU5nXb"
   },
   "source": [
    "### <b> Learning Objectives </b>\n",
    "By the end of this lesson, you will be able to:\n",
    "- Define ensemble learning\n",
    "- List different types of ensemble methods\n",
    "- Build an intuition\n",
    "- Apply different algorithms of ensemble learning using use cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wtzozj625qOm"
   },
   "source": [
    "### <b> What Is Ensemble Learning? </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Td7-rOBj5usF"
   },
   "source": [
    "Ensemble techniques combine individual models to improve the stability and predictive power of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wvVZB0oo54Ou"
   },
   "source": [
    "#### <b> Ideology Behind Ensemble Learning: </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uvb55H6-57G_"
   },
   "source": [
    "* Certain models do well in modeling one aspect of the data, while others do well in modeling another.\n",
    "\n",
    "* Instead of learning a single complex model, learn several simple models and combine their output to produce the final decision.\n",
    "\n",
    "* Individual model variances and biases are balanced by the strength of other models in ensemble learning.\n",
    "\n",
    "* Ensemble learning will provide a composite prediction where the final accuracy is better than the accuracy of individual models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WMhoybU359s1"
   },
   "source": [
    "#### <b> Working of Ensemble Learning </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LBaVCR3B6AOG"
   },
   "source": [
    "![Ensemble_Learning_Workflow](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/Applied_Machine_Learning/Images/Lesson_07_Ensemble_Learning/Ensemble_Learning_Workflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6KwXIaUo6E6e"
   },
   "source": [
    "#### <b> Significance of Ensemble Learning </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ml1ie17T6HcP"
   },
   "source": [
    "* Robustness\n",
    "  - Ensemble models incorporate the predictions from all the base learners\n",
    "* Accuracy\n",
    "  - Ensemble models deliver accurate predictions and have improved performances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VavpTzv26ON4"
   },
   "source": [
    "#### <b> Ensemble Learning Methods </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S151Fz-r6STd"
   },
   "source": [
    "* Techniques for creating an ensemble model\n",
    "* Combine all weak learners to form an ensemble, or create an ensemble of well-chosen strong and diverse models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jt9iwlMW6VPp"
   },
   "source": [
    "#### <b> Steps Involved in Ensemble Methods </b>\n",
    "\n",
    "Every ensemble algorithm consists of two steps:\n",
    "\n",
    "* Producing a cohort of predictions using simple ML algorithms\n",
    "* Combining the predictions into one aggregated model\n",
    "\n",
    "The ensemble can be achieved through several techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dfc4kJRf6Y8d"
   },
   "source": [
    "### <b> Types of Ensemble Methods </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pDRgzaPE6duh"
   },
   "source": [
    "#### <b> Averaging </b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DisGQkqI6gZP"
   },
   "source": [
    "![Averaging](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/Applied_Machine_Learning/Images/Lesson_07_Ensemble_Learning/Averaging.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "75i3XpeO6lmL"
   },
   "source": [
    "#### <b> Weighted Averaging </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cHaLDPhL6zM-"
   },
   "source": [
    "![Weighted_Averaging](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/Applied_Machine_Learning/Images/Lesson_07_Ensemble_Learning/Weighted_Averaging.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DGm8JLhT663p"
   },
   "source": [
    "### <b> Bagging Algorithms </b>\n",
    "\n",
    "Bootstrap Aggregation or bagging involves taking multiple samples from your training dataset (with replacement) and training a model for each sample.\n",
    "\n",
    "The final output prediction is averaged across the predictions of all of the submodels.\n",
    "\n",
    "The three bagging models covered in this section are as follows:\n",
    "\n",
    " - Bagged Decision Trees\n",
    " - Random Forest\n",
    " - Extra Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "COHCIquy7NOf"
   },
   "source": [
    "#### <b> 1. Bagged Decision Trees </b>\n",
    "\n",
    "Bagging performs best with algorithms that have a high variance. A popular example is decision trees, often constructed without pruning.\n",
    "\n",
    "Below, you can see an example of using the BaggingClassifier with the Classification and Regression Trees algorithm (DecisionTreeClassifier). A total of 100 trees are created.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xx8xgGfceF5u"
   },
   "source": [
    "- Scikit-learn is a Python library that provides a consistent interface for machine learning and statistical modeling, including classification, regression, clustering, and dimensionality reduction.\n",
    "- Pandas is a Python library for data manipulation and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagged Decision Trees for Classification\n",
    "import pandas\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
    "# names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "# dataframe = pandas.read_csv(url, names=names)\n",
    "# array = dataframe.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  6.   , 148.   ,  72.   , ...,   0.627,  50.   ,   1.   ],\n",
       "       [  1.   ,  85.   ,  66.   , ...,   0.351,  31.   ,   0.   ],\n",
       "       [  8.   , 183.   ,  64.   , ...,   0.672,  32.   ,   1.   ],\n",
       "       ...,\n",
       "       [  5.   , 121.   ,  72.   , ...,   0.245,  30.   ,   0.   ],\n",
       "       [  1.   , 126.   ,  60.   , ...,   0.349,  47.   ,   1.   ],\n",
       "       [  1.   ,  93.   ,  70.   , ...,   0.315,  23.   ,   0.   ]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"pima-indians-diabetes.csv\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pandas.read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = array[:,0:8]\n",
    "Y = array[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "tJuR-17J7QYl",
    "outputId": "e6ab3d1b-1352-4e0e-edc8-86166269805d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7578263841421736\n"
     ]
    }
   ],
   "source": [
    "cart = DecisionTreeClassifier()\n",
    "\n",
    "num_trees = 100\n",
    "model = BaggingClassifier(base_estimator = cart, \n",
    "                          n_estimators = num_trees,\n",
    "                          random_state = seed)\n",
    "\n",
    "results = model_selection.cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7747778537252221\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "\n",
    "num_trees = 1\n",
    "model = BaggingClassifier(base_estimator = lr, \n",
    "                          n_estimators = num_trees,\n",
    "                          random_state = seed)\n",
    "\n",
    "results = model_selection.cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Swj2Rvoa7IZF"
   },
   "source": [
    "#### <b> 2. Random Forest </b> \n",
    "\n",
    "Random forest is an extension of bagged decision trees.\n",
    "\n",
    "Samples of the training dataset are taken with replacement, but the trees are constructed in a way that reduces the correlation between individual classifiers. Specifically, rather than greedily choosing the best split point in the construction of the tree, only a random subset of features is considered for each split.\n",
    "\n",
    "You can construct a Random Forest model for classification using the RandomForestClassifier class.\n",
    "\n",
    "The example below provides a sample of Random Forest for classification with 100 trees and split points chosen from a random selection of three features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest Classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
    "# names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "# dataframe = pandas.read_csv(url, names=names)\n",
    "# array = dataframe.values\n",
    "\n",
    "# X = array[:,0:8]\n",
    "# Y = array[:,8]\n",
    "# seed = 7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "odZ4P_P37Uis",
    "outputId": "a180e4c4-1701-4366-aade-638770428d54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7747436773752563\n"
     ]
    }
   ],
   "source": [
    "num_trees = 100\n",
    "max_features = 3\n",
    "\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed, shuffle=True)\n",
    "model = RandomForestClassifier(n_estimators=num_trees, max_features=max_features)\n",
    "\n",
    "results = model_selection.cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FqyHO74A7ZOt"
   },
   "source": [
    "#### <b> 3. Extra Trees</b>\n",
    "\n",
    "Extra Trees are another modification of bagging where random trees are constructed from samples of the training dataset.\n",
    "\n",
    "You can construct an Extra Trees model for classification using the ExtraTreesClassifier class.\n",
    "\n",
    "The example below provides a demonstration of extra trees with a tree set of 100 and splits chosen from seven random features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extra Trees Classification\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
    "# names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "# dataframe = pandas.read_csv(url, names=names)\n",
    "# array = dataframe.values\n",
    "# X = array[:,0:8]\n",
    "# Y = array[:,8]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "xkf_0b5l7cq9",
    "outputId": "47fe251e-ffaa-4080-d018-24c0fdd87921"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7643198906356801\n"
     ]
    }
   ],
   "source": [
    "seed = 7\n",
    "\n",
    "num_trees = 100\n",
    "max_features = 7\n",
    "\n",
    "kfold = model_selection.KFold(n_splits=10, \n",
    "                              random_state=seed, \n",
    "                              shuffle=True)\n",
    "\n",
    "model = ExtraTreesClassifier(n_estimators=num_trees, \n",
    "                             max_features=max_features)\n",
    "\n",
    "results = model_selection.cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jlIFvFor7hmR"
   },
   "source": [
    "###  <b> Boosting Algorithms </b>\n",
    "\n",
    "Boosting ensemble algorithms create a sequence of models that attempts to correct the mistakes of the models before them in the sequence.\n",
    "\n",
    "Once created, the models make predictions that may be weighted by their demonstrated accuracy, and the results are combined to create a final output prediction.\n",
    "\n",
    "\n",
    "The two most common boosting ensemble machine learning algorithms are:\n",
    "\n",
    "- AdaBoost\n",
    "\n",
    "- Stochastic Gradient Boosting\n",
    "<br>\n",
    "\n",
    "#### <b> AdaBoost </b>\n",
    "\n",
    "AdaBoost was the first successful boosting ensemble algorithm. It generally works by weighting instances in the dataset by how easy or difficult they are to classify, allowing the algorithm to pay more or less attention to them in the construction of subsequent models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7aO-zHHTU1mN"
   },
   "source": [
    "![AdaBoost](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/Applied_Machine_Learning/Images/Lesson_07_Ensemble_Learning/AdaBoost.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QHCG5BI47oIg"
   },
   "source": [
    "You can construct an AdaBoost model for classification using the AdaBoostClassifier class.\n",
    "\n",
    "The example below demonstrates the construction of 30 decision trees in sequence using the AdaBoost algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AdaBoost Classification\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
    "# names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "\n",
    "# dataframe = pandas.read_csv(url, names=names)\n",
    "# array = dataframe.values\n",
    "# X = array[:,0:8]\n",
    "# Y = array[:,8]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "VmfduWVV7qeq",
    "outputId": "4817bcc8-3698-4195-e7c9-1a3bebac8eb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7552802460697198\n"
     ]
    }
   ],
   "source": [
    "seed = 7\n",
    "num_trees = 30\n",
    "\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed, shuffle=True)\n",
    "model = AdaBoostClassifier(n_estimators = num_trees, \n",
    "                           random_state=seed)\n",
    "\n",
    "results = model_selection.cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E2fRqjMx7tQk"
   },
   "source": [
    "#### <b> Stochastic Gradient Boosting </b>\n",
    "\n",
    "One of the most advanced ensemble approaches is Stochastic Gradient Boosting (also known as Gradient Boosting Machines). It's also a strategy that's proven to be one of the most effective methods for boosting performance via ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J5tYi9Q37wWu"
   },
   "source": [
    "#### <b> Steps of Gradient Boasting Machine </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vTeY0JU97zvU"
   },
   "source": [
    "![GBM_Steps](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/Applied_Machine_Learning/Images/Lesson_07_Ensemble_Learning/GBM_Steps.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ilE5qgrG7_FP"
   },
   "source": [
    "You can construct a Gradient Boosting model for classification using the **GradientBoostingClassifier** class.\n",
    "\n",
    "The example below demonstrates Stochastic Gradient Boosting for classification with 100 trees.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stochastic Gradient Boosting Classification\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
    "# names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "# dataframe = pandas.read_csv(url, names=names)\n",
    "\n",
    "# array = dataframe.values\n",
    "# X = array[:,0:8]\n",
    "# Y = array[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "yCYRA8lH8BQx",
    "outputId": "fc19a604-3b82-4b22-a3b0-e929bf7472e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7604921394395079\n"
     ]
    }
   ],
   "source": [
    "seed = 7\n",
    "num_trees = 100\n",
    "\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed, shuffle=True)\n",
    "model = GradientBoostingClassifier(n_estimators=num_trees, random_state=seed)\n",
    "results = model_selection.cross_val_score(model, X, Y, cv=kfold)\n",
    "\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vnyu4ovB5OjL"
   },
   "source": [
    "### <b>Voting Ensemble</b>\n",
    "\n",
    "Voting is one of the simplest ways of combining the predictions from multiple machine learning algorithms.\n",
    "\n",
    "It works by first creating two or more standalone models from your training dataset. A Voting Classifier can then be used to wrap your models and average the predictions of the submodels when asked to make predictions for new data.\n",
    "\n",
    "The predictions of the submodels can be weighted, but specifying the weights for classifiers manually or even heuristically is difficult. More advanced methods can learn how to best weight the predictions from submodels, but this is called stacking (stacked generalization) and is currently not provided in scikit-learn.\n",
    "\n",
    "You can create a voting ensemble model for classification using the **VotingClassifier** class.\n",
    "\n",
    "The code below provides an example of combining the predictions of logistic regression, classification, and regression trees and support vector machines together for a classification problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "wmMBPdN58cMW",
    "outputId": "ca81f02d-ec1a-46b1-a6b0-20c5c47cd309"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7681476418318524\n"
     ]
    }
   ],
   "source": [
    "#Voting Ensemble for Classification\n",
    "import pandas\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# url = \"pima-indians-diabetes.csv\"\n",
    "# names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "\n",
    "url = \"pima-indians-diabetes.csv\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pandas.read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "array\n",
    "\n",
    "dataframe = pandas.read_csv(url, names=names)\n",
    "\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "seed = 7\n",
    "kfold = model_selection.KFold(n_splits=10)\n",
    "\n",
    "#Create the sub models\n",
    "estimators = []\n",
    "model1 = LogisticRegression()\n",
    "estimators.append(('logistic', model1))\n",
    "\n",
    "model2 = DecisionTreeClassifier()\n",
    "estimators.append(('cart', model2))\n",
    "\n",
    "model3 = SVC()\n",
    "estimators.append(('svm', model3))\n",
    "\n",
    "model4 = RandomForestClassifier()\n",
    "estimators.append(('rf', model4))\n",
    "\n",
    "#Create the ensemble model\n",
    "ensemble = VotingClassifier(estimators)\n",
    "results = model_selection.cross_val_score(ensemble, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3xKjDdV7WeLM"
   },
   "source": [
    "**Note: In this lesson, we saw the use of the ensemble learning methods, and in the next lesson, we will be working on Recommender Systems.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x5yqWygG8lPX"
   },
   "source": [
    "![Simplilearn_Logo](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/Logo_Powered_By_Simplilearn/SL_Logo_1.png)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "7.01_Ensemble_Learning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
